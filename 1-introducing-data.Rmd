---
title: "Module 1 - Introducing Data"
subtitle: "edX EdinburghX Statistics"
author: "Clare Gibson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    theme: cosmo
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This module looks at the techniques for describing, presenting and summarising data. Throughout this module we will make use of a data set on primary school performance in Surrey and Hampshire to illustrate some of the key concepts.

We will make use of the `tidyverse` and `janitor` packages:
```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(janitor)
library(knitr)
```

## Loading the example data
Let's read in the data set we will use for this module and look at the first 6 rows of data. This data set describes Key Stage 2 performance for primary schools in Surrey for the 2018-2019 academic year. It was published by the UK government.
```{r read, message=FALSE, warning=FALSE}
performance <- read_csv("data-in/2018-2019_936_ks2final.csv") %>% 
  clean_names()

head(performance)
```

There are over 300 columns of data in this file. For the purposes of this module we can condense this down to a more manageable number of columns. We can also filter out any records that are not related to an individual school (i.e. records containing subtotals or totals).
```{r select}
performance <- performance %>% 
  filter(rectype == 1) %>% 
  select(urn,
         schname,
         nftype,
         reldenom,
         totpups,
         matprog,
         writprog,
         readprog)

head(performance)
```

Looking at the data types that R has determined, some columns have been read in as character types when they should be numeric. We can fix this for the `totpups`, `matprog`, `writprog` and `readprog` columns.
```{r data types, message=FALSE, warning=FALSE}
performance <- performance %>% 
  mutate(across(totpups:readprog, as.numeric))

head(performance)
```

## What is data?
Data is information and comes in different types:

* **Qualitative** data describes qualities. It is descriptive. There are two types of qualitative data:
    + **Ordinal** tells you about ordering (e.g. it ranks 8 out of 10)
    + **Nominal** simply describes or categorises something (e.g. it's pink)
* **Quantitative** data talks about quantities or numbers (e.g. 15cm high). There are two types of quantitative data:
    + **Discrete** can only take a set of possible values (e.g. £1.90)
    + **Continuous** can take on any value in a range (e.g. 12, 12.31. 12.34562)

Usually we are interested in data that comes from recording some quantity or quality of interest. We may know in advance what the possible values of the data could be, but until we collect the data we do not know the actual values. For example, we know that if we roll a dice we will get a number between 1 and 6, but we don't know which one until we roll it. We might know that the speed of a moving car at a particular location is between 0 and 100 miles per hour, but we don't know the exact speed until we measure it. When we want to measure a quantity or quality and it can take on a range of values like this, before we observe the actual value we refer to it as a **random variable**, or variable for short.

### What types of data can you find?
In our example data set we have the following fields and data types:

Column Name     Data Type               Description
--------------- ----------------------  -----------------
urn             Qualitative Nominal     Unique Reference Number of the school
schname         Qualitative Nominal     School name
nftype          Qualitative Nominal     School type
reldenom        Qualitative Nominal     Religious denomination
totpups         Quantitative Discrete   Total number of pupils
matprog         Quantitative Continuous Maths progress measure
writprog        Quantitative Continuous Writing progress measure
readprog        Quantitative Continuous Reading progress measure

Table: Fields and data types in the example data set

## What is statistics?
Statistics is a branch of maths that deals with analysing data and interpreting the results. Scientific research is often advanced by posing scientific questions and then answering these, and statistics answers those questions using data. For example, if you are looking at primary school performance, you might want to know if schools with a religious denomination achieve better maths results than those with no religious denomination. To answer this, you might first collect some data. For example, you could take a random sample of 100 schools; 50 of those have no religious denomination and 50 schools have a religious denomination. Then you would collect data on the maths performance of each school and use that data to determine whether having a religious denomination improves school performance in maths.

In order to answer the question, you would first look at the data, which might look something like this:

```{r maths performance}
# We'll create a new df called test_reldenom to hold our sample data for
# religious denomination and maths performance.
test_reldenom <- performance %>% 
  # First we'll select only the columns we need
  select(urn, reldenom, matprog) %>% 
  # Next we'll filter out any NA values in matprog
  filter(!is.na(matprog)) %>% 
  # Now let's create a new binary variable which determines if the school has
  # a religious denomination or not
  mutate(is_religious = if_else(
    reldenom %in% c("None", "Does not apply"),
    FALSE,
    TRUE)) %>% 
  # Now we'll subset the df so that we have two equal groups of 50 schools,
  # comparing religious against non-religious schools
  group_by(is_religious) %>% 
  slice_sample(n = 50) %>% 
  ungroup() %>% 
  # We can remove the reldenom column and re-order the remaining columns
  select(urn, is_religious, matprog)

head(test_reldenom)
```

The table shows 100 schools, and the corresponding maths progress score for each. You can then divide them into 2 groups and create a chart with one plot for the group with a religious denomination and one plot for the group without a religious denomination, as shown below.
```{r boxplot}
ggplot(test_reldenom, aes(is_religious, matprog)) +
  geom_boxplot()
```

Looking at this data might tell us that having a religious denomination has possibly increased the maths progress score, but we would need to do a formal statistical analysis, which will take into account the different amounts of variability, to work this out for certain.

### The statistics cycle
Often we start out with an initial question that we want to answer with data. We collect the data and analyse it to answer that question, but in doing so that uncovers extra questions that we may want to go further and investigate. This process of analysis, refinement and reanalysis is generally how scientific research develops. We call this diagram the statistics cycle as shown below.

![](images/1-01.png)

It is a useful tool for designing, conducting and interpreting a statistical experiment. We typically start with a question that we wish to answer. Given the question we collect relevant data that allows us to investigate this question. Using the data we conduct an appropriate statistical analysis and interpret the results accordingly. Given our updated understanding of the world from the results obtained we may pose a further follow-up or in-depth question that we wish to investigate, and so on...

## Designing an experiment
The climate of the Earth has changed throughout history. There is evidence to show that we are currently in a state of rapid warming, and there is a lot of debate about whether or not this will continue, and what impact it might have on our planet. Scientific studies looking into global warming make use of statistics in order to try to answer these difficult questions. A huge amount of data is needed to say anything concrete about long-term weather patterns, but closer to home we can investigate the temperature of our local area and how this has changed over the past few years.

### Designing a weather study
First, we need to design a study that will allow us to answer our question. We begin by defining the specific question that we are interested in, before looking at what data we might collect, and how we might record it.

We are interested in whether or not there has been an overall change in temperature over the past three years, and if so what this change has been.

Here is how we might start to design our experiment:

* **Question to pose:** How has the monthly average daily maximum temperature changed over the last three years?
* **How to collect the data:** Obtain a list of the monthly average maximum temperatures over the past three years.
* **How to record the data:** As a number, in degrees Celsius, rounded to the nearest degree.

### Collecting weather data
Now that we have designed a study, we are ready to collect data. Here are the monthly average maximum temperatures for Edinburgh over the three years from January 2014 to December 2016 (source: [Weather Underground](www.wunderground.com)).
```{r Edinburgh weather}
year <- c(rep(2012, 12), rep(2013, 12), rep(2014, 12))
month <- rep(c("Jan", "Feb", "Mar", "Apr",
               "May", "Jun", "Jul", "Aug",
               "Sep", "Oct", "Nov", "Dec"), 3)
temp <- c(7, 8, 10, 12, 15, 18, 21, 18, 17, 14, 10, 7,
          7, 7, 9, 13, 13, 17, 17, 19, 17, 14, 11, 10,
          7, 7, 9, 10, 15, 16, 19, 18, 18, 13, 8, 9)

edinburgh <- tibble(year, month, temp) %>% 
  mutate(year = as.character(year),
         month = factor(month, levels = month.abb))

kable(edinburgh %>% 
        pivot_wider(names_from = month, values_from = temp))
```

### What does the data tell us?
It is hard to distinguish any firm patterns just by looking at this table of data. We can see that May to September is typically warmer than October to April. We can’t really tell if the trend by year is increasing or decreasing temperatures.

# Visualising data
We just looked at weather data in the form of a table. We found that it was not simple to extract useful information from the data in this form, even though it was a relatively small data set. Modern data science often involves analysing data sets with thousands, if not millions of pieces of data in them. This section focuses on ways of visualising data in order to make it easier to analyse and interpret. This idea dates back to the work of William Playfair in the late 1700s.

In 1786, the Scottish engineer and economist William Playfair (1759-1823) published a book of economical data called The Commercial and Political Atlas, in which the tables of data were also presented in the form of line graphs. Today, the line graph is still largely the favoured way of presenting financial data over time.

The Financial Times Stock Exchange 100 Index (FTSE 100) gives a measure of the value of the UK's 100 largest qualifying companies. It is calculated and published every 15 seconds between 08:00 and 16:30, Monday to Friday, giving rise to a wealth of financial data. The line graph below shows the value of the FTSE 100 at the end of each trading day over September 2016.

![](images/1-02.png)

The graph has a horizontal scale showing the dates in September, and a vertical scale showing a range of FTSE 100 values. Each dot on the graph represents one piece of data - the value of the FTSE 100 at the end of a given working day. Reading vertically downwards off the horizontal scale tells you the date, while reading horizontally to the left off the vertical scale tells you the value of the FTSE 100 at the end of that working day. The dot furthest to the left, for example, tells you that the value of the FTSE 100 at the end of the 1st September was just less than 6750. The dots have been joined by a line to help provide a visualisation of how the FTSE 100 value has changed, producing the line graph.

## Line graphs
**Line graphs** can be used whenever we want to show how a variable changes over time. The graph below shows a line graph of three data sets showing the monthly average daily maximum temperature of Edinburgh in the years 2012, 2013 and 2014.
```{r line chart}
ggplot(data = edinburgh) +
  geom_line(mapping = aes(x = month, y = temp, group = year, color = year))
```

## Bar charts
Playfair's *Statistical Breviary*, published in 1801, used entirely different data visualisation methods to the Commercial and Political Atlas. One of these is the **bar chart**, which Playfair used to show and compare the populations and revenues of different countries. The bar chart is one of the most commonly seen types of graph or chart today, and is a useful way of showing and comparing amounts or values across different categories.

We have data on school types in Surrey. The bar chart below shows the total number of schools by school type.
```{r bar chart, message=FALSE, warning=FALSE}
ggplot(data = performance) +
  geom_bar(mapping = aes(x = reorder(nftype, -totpups), y = totpups),
           stat = "identity",
           na.rm = TRUE) +
  labs(x = "Type of school",
       y = "Total pupils")
```

